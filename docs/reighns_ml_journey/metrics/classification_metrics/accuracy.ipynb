{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "J-IsLgX9IMZJ"
      },
      "outputs": [],
      "source": [
        "!pip install -q scikit-learn==1.0.1\n",
        "from sklearn import metrics\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JqcXlMcIzrH"
      },
      "source": [
        "### Definition\n",
        "\n",
        "!!! success \"Definition\"\n",
        "    Formally, if $\\hat{y}^{(i)}$ is the predicted value of the i-th sample and the ground truth is $y^{(i)}$, then accuracy can be defined as the fraction of predictions that our classifier/hypothesis/model predicted correctly, over the total number of samples in question.\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\text{accuracy}(\\hat{y}^{(i)}, y^{(i)}) &= \\dfrac{1}{\\text{num_samples}}\\sum_{i=1}^{\\text{num_samples}}\\mathrm{1}(y^{(i)}\\hat{y}^{(i)}) \\\\ \n",
        "                                        &= \\dfrac{\\text{Number of correctly classified cases}}{\\text{Number for all cases}}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "where $\\mathrm{1}(x)$ is the [indicator function](https://en.wikipedia.org/wiki/Indicator_function).\n",
        "\n",
        "\n",
        "!!! note\n",
        "    Accuracy is a simple enough metric such that the definition for both binary and multiclass classification is the same."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGrNW52c790E",
        "tags": []
      },
      "source": [
        "### When to use Accuracy as a metric\n",
        "\n",
        "Classes are well balanced: Accuracy is a valid choice of evaluation for classification problems which are well balanced and not skewed or no class imbalance. Typically, one should plot EDA and see the classes - if they are roughly equal, then `accuracy` can be used. However, accuracy should not be the only metric to look at in a classification problem.\n",
        "\n",
        "\n",
        "### When NOT to use Accuracy as a metric\n",
        "\n",
        "???+ danger\n",
        "    Consider an imbalanced set, where the training data set has 100 patients (data points), and the ground truth is 90 patients are of class = 0, which means that these patients do not have cancer, whereas the remaining 10 patients are in class 1, where they do have cancer. This is an example of class imbalance where the ratio of class 1 to class 0 is $1:9$.\n",
        "\n",
        "    Next, we consider **a baseline (almost trivial) classifier**:\n",
        "\n",
        "    ```python\n",
        "    def baselineModel(patient_data):\n",
        "            training...\n",
        "        return benign\n",
        "    ```\n",
        "\n",
        "    where we predict the patient's class as the most frequent class. Meaning, the most frequent class in this question is the class = 0, where patients do not have cancer, so we just assign this class to everyone in this set. By doing this, we will inevitably achieve a **in-sample** accuracy rate of $\\frac{90}{100} = 90\\%$. But unfortunately, this supposedly high accuracy value is completely useless, because this classifier did not label any of the cancer patients correctly.\n",
        "\n",
        "    The consequence can be serious, assuming the test set has the same distribution as our training set, where if we have a test set of 1000 patients, there are 900 negative and 100 positive. Our model just literally predict every one of them as benign, yielding a $90\\%$ **out-of-sample** accuracy.\n",
        "\n",
        "    What did we conclude? Well, for one, our `accuracy` can be 90% high and looks good to the laymen, but it failed to predict the most important class of people.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1O0tyNnO790E"
      },
      "source": [
        "### Implementation of Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LmF7MJfX790F"
      },
      "outputs": [],
      "source": [
        "def accuracy(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
        "    \"\"\"Calculates accuracy score of a prediction.\n",
        "\n",
        "    Can be used for both binary and multiclass classification.\n",
        "\n",
        "    Args:\n",
        "        y_true (np.ndarray): the correct labels, shape (n_samples, )\n",
        "        y_pred (np.ndarray): the predicted labels, shape (n_samples, )\n",
        "\n",
        "    Returns:\n",
        "        accuracy_score (float): the accuracy score\n",
        "    \"\"\"\n",
        "\n",
        "    accuracy_count = 0  # numerator\n",
        "    num_samples = len(y_true)  # denominator\n",
        "\n",
        "    for y_t, y_p in zip(y_true, y_pred):\n",
        "        if y_t == y_p:\n",
        "            accuracy_count += 1\n",
        "\n",
        "    accuracy_score = accuracy_count / num_samples\n",
        "    return accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zr3IrGz6J8JM",
        "outputId": "5aaa4c2b-8cca-4104-a099-253f45389f3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hn accuracy: 0.6666666666666666\n",
            "sklearn accuracy: 0.6666666666666666\n",
            "hn accuracy: 0.5\n",
            "sklearn accuracy: 0.5\n"
          ]
        }
      ],
      "source": [
        "# Binary Classification\n",
        "y_true = np.asarray([1,1,0,1,0,0])\n",
        "y_pred = np.asarray([1,1,1,0,0,0])\n",
        "print(f\"hn accuracy: {accuracy(y_true, y_pred)}\")\n",
        "print(f\"sklearn accuracy: {metrics.accuracy_score(y_true, y_pred, normalize=True)}\")\n",
        "\n",
        "# Multiclass Classification\n",
        "y_pred = np.asarray([0, 2, 1, 3])\n",
        "y_true = np.asarray([0, 1, 2, 3])\n",
        "print(f\"hn accuracy: {accuracy(y_true, y_pred)}\")\n",
        "print(f\"sklearn accuracy: {metrics.accuracy_score(y_true, y_pred, normalize=True)}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "accuracy.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
