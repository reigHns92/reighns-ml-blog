{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98aa06e6-fc07-468e-a1d3-583d5655e08d",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\R}{\\mathbb{R}}\n",
    "\\newcommand{\\v}{\\mathbf{v}}\n",
    "\\newcommand{\\a}{\\mathbf{a}}\n",
    "\\newcommand{\\b}{\\mathbf{b}}\n",
    "\\newcommand{\\c}{\\mathbf{c}}\n",
    "\\newcommand{\\w}{\\mathbf{w}}\n",
    "\\newcommand{\\u}{\\mathbf{u}}\n",
    "\\newcommand{\\x}{\\mathbf{x}}\n",
    "\\newcommand{\\y}{\\mathbf{y}}\n",
    "\\newcommand{\\z}{\\mathbf{z}}\n",
    "\\newcommand{\\0}{\\mathbf{0}}\n",
    "\\newcommand{\\1}{\\mathbf{1}}\n",
    "\\newcommand{\\A}{\\mathbf{A}}\n",
    "\\newcommand{\\B}{\\mathbf{B}}\n",
    "\\newcommand{\\rank}{\\textbf{rank}}\n",
    "\\newcommand{\\P}{\\mathbb{P}}\n",
    "\\newcommand{\\E}{\\mathcal{F}}\n",
    "\\newcommand{\\S}{\\Omega}\n",
    "\\newcommand{\\C}{\\mathbf{C}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd68bc5-8150-4df4-bab9-0eab506d4871",
   "metadata": {},
   "source": [
    "## Baye's Theorem and the Law of Total Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92605331-b2a5-4e44-b5d1-5c2006ff603f",
   "metadata": {},
   "source": [
    "### Definition (Conditional Probability)\n",
    "\n",
    "As a refresher, we re-state the definition of conditional probability here.\n",
    "\n",
    "Let $\\P$ be a probability function and $A, B \\in \\E$ be events. Then the **conditional probability** of $A$ given that event $B$ has occurred is denoted \n",
    "\n",
    "$$\n",
    "\\P(A|B)\n",
    "$$\n",
    "\n",
    "and is defined by\n",
    "\n",
    "$$\n",
    "\\P(A|B) = \\dfrac{\\P(A\\cap B)}{\\P(B)}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Stanley H. Chan: Introduction to Probability for Data Science, 2021. (pp. 81-84)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb604eb-92c0-4d04-9373-a63c6622d6fd",
   "metadata": {},
   "source": [
    "### Definition (Baye's Theorem)\n",
    "\n",
    "Two events $A, B \\in \\E$ such that $\\P(A) > 0$ and $\\P(B) > 0$, we have\n",
    "\n",
    "$$\n",
    "\\P(A|B) = \\dfrac{\\P(B|A)\\P(A)}{\\P(B)}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Stanley H. Chan: Introduction to Probability for Data Science, 2021. (pp. 89)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1d39ad-93dd-40fd-9f06-37b3ad99d260",
   "metadata": {},
   "source": [
    "#### Intuition (Baye's Theorem)\n",
    "\n",
    "The observant reader should find it easy that this formula is deduced by abusing the fact that $\\P(A \\cap B) = \\P(B \\cap A)$, and therefore \n",
    "\n",
    "$$\n",
    "\\P(A|B) = \\dfrac{\\P(A\\cap B)}{\\P(B)} = \\dfrac{\\P(B \\cap A)}{\\P(B)} = \\dfrac{\\P(B ~|~ A)\\P(A)}{\\P(B)}\n",
    "$$\n",
    "\n",
    "since $\\P(B ~|~ A) = \\dfrac{\\P(B \\cap A)}{\\P(A)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac55649-b75d-42ac-988d-62ed8ab8e7b0",
   "metadata": {},
   "source": [
    "### Terminology (Posterior and Conditional Probability)\n",
    "\n",
    "From the proof of **Baye's Theorem**, we see that there are two ways to view $\\P(A \\cap B)$. If the context is clear, we may call $\\P(B|A)$ the **conditional probability** and $\\P(A|B)$ the **posterior probability**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873c2c78-1858-4f0e-a60c-a713ad1f7442",
   "metadata": {},
   "source": [
    "### Theorem (Law of Total Probability)\n",
    "\n",
    "Let $\\{A_1, \\ldots, A_n\\}$ be a *partition* of the sample space $\\S$. This means that $A_1, \\ldots, A_n$ are *disjoint* and $\\S = A_1 \\cup \\cdots \\cup A_n$. Then, for any $B \\subseteq \\S$, we have\n",
    "\n",
    "$$\n",
    "\\P(B) = \\sum_{i=1}^{n} \\P(B|A_i)\\P(A_i)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Stanley H. Chan: Introduction to Probability for Data Science, 2021. (pp. 89-90)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd51a5ac-3704-42aa-a8a2-de2a4cbcaf5f",
   "metadata": {},
   "source": [
    "### Intuition and Interpretation (Law of Total Probability)\n",
    "\n",
    "The law of total probability can be stated more intuitively as the sum of event $B$ and each of the disjoint event $A_i$. We have to first recognize that the image below makes sense, assumming that the whole sample space can be made up of 3 disjoint events $A_1, A_2, A_3$, and we see that event $B$ touches all of them (i.e. there are overlaps), then it is obvious from the figure that $\\P(B)$ is the sum of $\\P(B \\cap A_i)$. Note that event $B$ does not necessarily need to touch all $A_i$, if it doesn't, then $\\P(B \\cap A_i) = 0$.\n",
    "\n",
    "Now if this intuition is established, we just express $\\P(B \\cap A_i) = \\P(B|A_i)\\P(A_i)$ by conditional probability and we recovered back the formula for the law of total probability.\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/probability_and_statistics/02_introduction_to_probability/law_of_total_probability.jpg\" style=\"width:500px;height:600px;margin-left:auto; margin-right:auto\"/>\n",
    "<p style=\"height=300 text-align: center\">\n",
    "    <b>Fig 1: Partition of Law of Total Probability</b>\n",
    "</p>\n",
    "\n",
    "> The intuition of this is also apparent after understanding that if a sample space can be broken down into disjoint events $A_1, A_2, \\ldots, A_k$, then any event $B$ that is in the sample space must touch some of these disjoint events, whenever they overlap, we can represent them as $\\P(A_i \\cap B)$, and if we add them up it is just the event $B$ itself. If they don't overlap, we can still add them anyways since $P(A_k \\cap B) = 0$. Note the loose usage of the index here, $i$ means overlap, $k$ means non-overlap.\n",
    "\n",
    "---\n",
    "\n",
    "**Stanley H. Chan: Introduction to Probability for Data Science, 2021. (pp. 90)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1608000-4eb8-4cb2-b3d0-f0b76d3e888b",
   "metadata": {},
   "source": [
    "### Corollary (Law of Total Probability)\n",
    "\n",
    "Let $\\{A_1, \\ldots, A_n\\}$ be a *partition* of the sample space $\\S$. This means that $A_1, \\ldots, A_n$ are *disjoint* and $\\S = A_1 \\cup \\cdots \\cup A_n$. Then, for any $B \\subseteq \\S$, we have\n",
    "\n",
    "$$\n",
    "\\P(A_j|B) = \\dfrac{\\P(B|A_j)\\P(A_j)}{\\sum_{i=1}^{n}\\P(B|A_i)\\P(A_i)}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Stanley H. Chan: Introduction to Probability for Data Science, 2021. (pp. 90)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af19db65-040e-463e-b51f-c777a6beda2d",
   "metadata": {},
   "source": [
    "### Exercise 2.47\n",
    "\n",
    "**Stanley H. Chan: Introduction to Probability for Data Science, 2021. (pp. 90-91)**\n",
    "\n",
    "### Exercise 2.48\n",
    "\n",
    "**Stanley H. Chan: Introduction to Probability for Data Science, 2021. (pp. 91)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
