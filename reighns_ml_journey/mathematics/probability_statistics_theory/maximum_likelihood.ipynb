{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32eb26c2-e1bb-4b5f-a279-eb88cf3bc084",
   "metadata": {},
   "source": [
    "$$\\newcommand{\\F}{\\mathbb{F}}\n",
    "\\newcommand{\\R}{\\mathbb{R}}\n",
    "\\newcommand{\\v}{\\mathbf{v}}\n",
    "\\newcommand{\\a}{\\mathbf{a}}\n",
    "\\newcommand{\\b}{\\mathbf{b}}\n",
    "\\newcommand{\\c}{\\mathbf{c}}\n",
    "\\newcommand{\\w}{\\mathbf{w}}\n",
    "\\newcommand{\\u}{\\mathbf{u}}\n",
    "\\newcommand{\\x}{\\mathbf{x}}\n",
    "\\newcommand{\\y}{\\mathbf{y}}\n",
    "\\newcommand{\\z}{\\mathbf{z}}\n",
    "\\newcommand{\\0}{\\mathbf{0}}\n",
    "\\newcommand{\\1}{\\mathbf{1}}\n",
    "\\newcommand{\\A}{\\mathbf{A}}\n",
    "\\newcommand{\\B}{\\mathbf{B}}\n",
    "\\newcommand{\\rank}{\\textbf{rank}}\n",
    "\\newcommand{\\P}{\\mathcal{P}}\n",
    "\\newcommand{\\C}{\\mathbf{C}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dfdd85-346d-4e95-8a1a-14ef9e8e8784",
   "metadata": {},
   "source": [
    "Density estimation is the problem of estimating the probability distribution for a sample of\n",
    "observations from a problem domain. There are many techniques for solving density estimation,\n",
    "although a common framework used throughout the field of machine learning is maximum\n",
    "likelihood estimation. Maximum likelihood estimation involves defining a likelihood function\n",
    "for calculating the conditional probability of observing the data sample given a probability\n",
    "distribution and distribution parameters. This approach can be used to search a space of possible\n",
    "distributions and parameters. This flexible probabilistic framework also provides the foundation\n",
    "for many machine learning algorithms, including important methods such as linear regression\n",
    "and logistic regression for predicting numeric values and class labels respectively, but also more\n",
    "generally for deep learning artificial neural networks. In this tutorial, you will discover a gentle\n",
    "introduction to maximum likelihood estimation. After reading this tutorial, you will know:\n",
    " Maximum Likelihood Estimation is a probabilistic framework for solving the problem of\n",
    "density estimation.\n",
    " It involves maximizing a likelihood function in order to find the probability distribution\n",
    "and parameters that best explain the observed data.\n",
    " It provides a framework for predictive modeling in machine learning where finding model\n",
    "parameters can be framed as an optimization problem.\n",
    "Let’s get started.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3cbfa3-7c25-4500-b562-766cc14c140a",
   "metadata": {},
   "source": [
    "Probability for Machine Learning - Jason Brownlee"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672845a1-c657-435e-939b-97e163ecc9c6",
   "metadata": {},
   "source": [
    "### Intuition for Probability Distribution Function\n",
    "\n",
    "\n",
    "\n",
    "First you need to be very clear about what a **probability distribution** is. Consider that we have 10 students and we model their marks where the full marks of the test is 16/16. \n",
    "\n",
    "- Define a random variable $X$ where $X$ represents the marks of each student.\n",
    "- Assume further that this **random variable** $X$ is following a **normal distribution** with $\\mu = 11$ and $\\sigma = 3$, can we find the probability distribution for the marks of the whole cohort (10 students)? Yes we can, because we have the parameters of the distribution. If you do not know what is the meaning of parameters, please go revise on it[^statistical_parameter], it is very important for you to understand that the **parameter** decides the probability distribution of any model. \n",
    "- Recall the general formula for the **PDF** of the normal distribution is\n",
    "\n",
    "    $$f(X = x) = \\dfrac{e^{-(x-\\mu)^2}/(2\\sigma^2)}{\\sigma \\sqrt{2\\pi}}$$\n",
    "    \n",
    "- And in normal distribution once we have the mean and standard deviation of the dataset, we can recover the whole pdf of the model, hence the mean and standard deviation are our parameters.  So let us say we want to find $P(11 < X < 13~|~\\mu = 11, \\sigma = 3)$, we can easily find it to be around $0.31 = 31\\%$, we can basically find any probabilities **as long as we are given the parameters**. So, **we must have the correct mindset that** probability density functions (or pmf alike) are legitimate functions that takes in any $X = x$ and outputs the probability of this $x$ **happening**. (Of course in continuous distribution we are usually only interested in the range of $x$, but for the purpose of intuition, we do not need to be so pedantic).\n",
    "\n",
    "---\n",
    "\n",
    "For the more mathematically formal people, here is the more precise definition of what I described above:\n",
    "\n",
    "Suppose you have random variables $X$ which arise from a parameterized distribution $\\P(X; \\theta)$, where $\\theta$ is the parameter characterizing the distribution $\\P$.  Then the probability of $X = x$ would be: $P(X = x~|~\\theta) = \\P(x; \\theta)$, with known $\\theta$.  \n",
    "\n",
    "[^statistical_parameter]: https://en.wikipedia.org/wiki/Statistical_parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f034d493-4168-411d-bb4d-a13aa899041e",
   "metadata": {},
   "source": [
    "### Likelihood Function\n",
    "\n",
    "However, in the real world setting, more often than not, we have the data $X$, like we have conveniently the scores of all the 10 students above, which **could be a random sample taken** from the whole school's population. Now we are tasked to find the probability distribution of the whole population (say 10,000 students), and we would have calculated it ever so easily **if we knew what the parameters were!** Unfortunately we do not have the true parameters.\n",
    "\n",
    "Our main motivation now is to find the **parameter**, because without it, we cannot complete the task of finding the distribution of the population. We can never know the real/true parameter $\\theta = (\\mu, \\sigma)$, but we can obtain a good estimate of it by making use of the data that we do have! In this scenario we were given 10 data points (in real life it is usually much more), say the 10 data points are $$\\mathbf{X} = [3,9,4,10,12,16,5,11,9,9]$$\n",
    "\n",
    "So we do a sleight of hand using our original **probability density function**, $P(X = x~|~ \\theta)$. Instead of being a function of $X = x$ where $\\theta$ is known, we instead let $X = x$ be fixed, and let $\\theta$ be the variable now. The idea is that this function now is \\textbf{NO LONGER a function of} $X=x$, and is instead a function of $\\theta$, where it takes in all possible values of $\\theta$, and outputs a value called the \\textbf{likelihood value.} So now, in a less informal way, our new function looks like $$P(\\mathbf{X} = [3,9,4,10,12,16,5,11,9,9]~|~ \\theta)$$ and it means \\textbf{what is the probability of OBSERVING these data points, given different values of theta.} One needs to plot the graph of likelihood out to get a better idea (wikipedia).   \\bigskip\n",
    "\n",
    "\n",
    "    \n",
    "\\bigskip\n",
    "\n",
    "So imagine our function (plot likelihood value vs parameter) has a local/global maximum, and that maximum is what we are finding ultimately. Because it is reasonable for us to believe that, the \\textbf{parameter} that gives us the maximum value of $P(\\mathbf{X} = [3,9,4,10,12,16,5,11,9,9]~|~ \\theta)$ will suggest that \\textbf{given these 10 data points}, this $\\theta$ that we just found, gives us the most \\textbf{likelihood/probability} that these 10 points are actually observed. \n",
    "\n",
    "\\bigskip\n",
    "\n",
    "We formally define this function to be $$\\mathcal{L}(\\theta~|~ X = x) = P(X = x~|~\\theta) $$\n",
    "\n",
    "\\bigskip\n",
    "\n",
    "I cannot emphasize enough that even those the likelihood function $\\mathcal{L}$ and the probability function $P$ have the exact same form, they are fundamentally different in which one is a function of the parameter $\\theta$, and the other is a function of the data points $X = x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f1170e-e858-4652-b25a-25be7cd95e4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
