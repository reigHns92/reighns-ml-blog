{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e167d03c-63fd-45ad-9df9-d157c5e8e2aa",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "\\newcommand{\\ytrue}{\\mathbf{y_{\\textbf{true}}}}\n",
    "\\newcommand{\\yprob}{\\mathbf{y_{\\textbf{prob}}}}\n",
    "\\newcommand{\\y}{\\mathbf{y}}\n",
    "\\newcommand{\\x}{\\mathbf{x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace3b52d-2123-48bc-9808-211b9aba5450",
   "metadata": {},
   "source": [
    "## Imports and Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6c7cceec-cb9c-4745-9dc9-4f16812334d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.10.1+cpu'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e06e77-bbdb-4960-9315-ad1e29aa21ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_to_np(tensor: torch.Tensor) -> np.ndarray:\n",
    "    \"\"\"Convert a PyTorch tensor to a numpy array.\n",
    "\n",
    "    Args:\n",
    "        tensor (torch.Tensor): The PyTorch tensor to convert.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The converted numpy array.\n",
    "    \"\"\"\n",
    "    return tensor.detach().cpu().numpy()\n",
    "\n",
    "def compare_equality_two_tensors(\n",
    "    tensor1: torch.Tensor, tensor2: torch.Tensor\n",
    ") -> bool:\n",
    "    \"\"\"Compare two PyTorch tensors for equality.\n",
    "\n",
    "    Args:\n",
    "        tensor1 (torch.Tensor): The first PyTorch tensor to compare.\n",
    "        tensor2 (torch.Tensor): The second PyTorch tensor to compare.\n",
    "\n",
    "    Returns:\n",
    "        bool: Whether the two tensors are equal.\n",
    "    \"\"\"\n",
    "    if torch.all(torch.eq(tensor1, tensor2)):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def compare_closeness_two_tensors(\n",
    "    tensor1: torch.Tensor,\n",
    "    tensor2: torch.Tensor,\n",
    "    epsilon: float,\n",
    "    *args,\n",
    "    **kwargs\n",
    ") -> bool:\n",
    "    \"\"\"Compare two PyTorch tensors for closeness.\n",
    "\n",
    "    Args:\n",
    "        tensor1 (torch.Tensor): The first PyTorch tensor to compare.\n",
    "        tensor2 (torch.Tensor): The second PyTorch tensor to compare.\n",
    "        epsilon (float): The epsilon value to use for closeness.\n",
    "\n",
    "    Returns:\n",
    "        bool: Whether the two tensors are close.\n",
    "    \"\"\"\n",
    "    if torch.allclose(tensor1, tensor2, atol=epsilon, *args, **kwargs):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dd40b6-14c7-493d-aad6-46c395423767",
   "metadata": {},
   "source": [
    "## Cross-Entropy as a Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf4699d-729a-4d0e-a6d4-ccd9ab4aed3e",
   "metadata": {},
   "source": [
    "### Cross-Entropy Setup\n",
    "\n",
    "We first understand the idea and intuition of **Cross-Entropy Loss** on one single example. Consider a dataset of cat (class 0) and dogs (class 1) where after one hot encoding we have class 0 to be $[1, 0]$ and class 1 to be $[0, 1]$.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d996a558-3056-41e0-b02a-77234ee0632d",
   "metadata": {},
   "source": [
    "\n",
    "We first understand the idea and intuition of **Cross-Entropy Loss** on one single example. Consider a dataset of cat (class 0) and dogs (class 1) where after one hot encoding we have class 0 to be $[1, 0]$ and class 1 to be $[0, 1]$.\n",
    "\n",
    "We are given the following:\n",
    "\n",
    "- $\\mathcal{D}$: The dataset $\\mathcal{D} = (\\mathcal{X}, \\mathcal{y})$.\n",
    "- $\\mathcal{X}$: This is a $2 \\times 3 \\times 224 \\times 224$ tensor of the shape $B \\times C \\times H \\times W$. This can be understood as 2 RGB images of size 224. \n",
    "- $\\mathcal{y}$: This is the corresponding ground truth one-hot encoded matrix, we have cat, dog and pig respectively (3 classes): \n",
    "    $$\\ytrue = \\begin{bmatrix}  1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}$$\n",
    "- $\\mathbf{x}_i$: We represent each $x_{i}$ as the $i$-th image. This can be a **random variable**. Image 1 is a cat, and Image 2 is a pig.\n",
    "- $\\mathbf{y}_i$: The corresponding label for the $i$-th sample/image, for sample 1, it is $\\begin{bmatrix} 1 & 0 & 0 \\end{bmatrix}$. \n",
    "- $P$: The probability distribution for the ground truth target - when it is $[1, 0, 0]$, one can understand it as the distribution where cat's probability is 1, and dog's probability is 0 and pig 0.\n",
    "- $Q$: The probability distribtion of the estimate on the $\\mathbf{x}_i$, which is say, $[0.9, 0.01, 0.09]$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b33fc115-9462-43f2-bdf8-545c1fe077a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 0, 0],\n",
      "        [0, 0, 1]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.zeros(size=(2, 3, 224, 224), dtype=torch.float32)\n",
    "y_true = torch.tensor([0, 2], dtype=torch.long)\n",
    "y_true_ohe = torch.tensor([[1, 0, 0], [0, 0, 1]], dtype=torch.long)\n",
    "print(y_true_ohe)\n",
    "compare_equality_two_tensors(y_true_ohe, torch.nn.functional.one_hot(y_true, num_classes=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc55241-bc7e-40cf-8ee4-84a5a7b67af6",
   "metadata": {},
   "source": [
    "### Z Logits\n",
    "\n",
    "Then we have the following, assume a hypothesis $h_{\\theta}(\\mathcal{X})$ on the dataset $\\mathcal{X}$ and it outputs logits of the form, note that we are passing in inputs of shape $(2, 3, 224, 224)$ but after some transformations we have the logits to be shape $(2, 3)$:\n",
    "\n",
    "- **z_logits:** $$\\textbf{z_logits} = \\begin{bmatrix} 1 & 2 & 3 \\\\ 2 & 4 & 6 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7d418278-e9d0-4727-ba14-12c8c0c5191a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [2., 4., 6.]])\n"
     ]
    }
   ],
   "source": [
    "z_logits = torch.tensor([[1, 2, 3], [2, 4, 6]], dtype=torch.float32)\n",
    "print(z_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0317504e-e8ba-4505-bcfb-1a2dd3b94d87",
   "metadata": {},
   "source": [
    "### Softmax[^softmax]\n",
    "\n",
    "The softmax function takes as input a vector $\\mathbf{z}$ of $K$ real numbers, and normalizes it into a probability distribution consisting of $K$ probabilities proportional to the exponentials of the input numbers. That is, prior to applying softmax, some vector components could be negative, or greater than one; and might not sum to 1; but after applying softmax, each component will be in the interval of 0 and 1, and the components will add up to 1, so that they can be interpreted as probabilities. Furthermore, the larger input components will correspond to larger probabilities. \n",
    "\n",
    "---\n",
    "\n",
    "The standard (unit) softmax function \n",
    "\n",
    "$$\\sigma : \\mathbb{R}^K\\to (0,1)^K$$\n",
    "\n",
    "is defined when $K$ is greater than one by the formula:\n",
    "\n",
    "$$\n",
    "\\sigma(\\mathbf{z})_i = \\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}} \\ \\ \\ \\ \\text{ for } i = 1, \\dotsc , K \\text{ and } \\mathbf z=(z_1,\\dotsc,z_K) \\in \\mathbb{R}^K\n",
    "$$\n",
    "\n",
    "In linear algebra notation, the $\\sigma$ soft(arg)max function takes in a **real vector** $\\mathbf{z}$ from the $K$ dimensional space, indicating that the vector $\\mathbf{z} \\in \\mathbb{R}^K$ has $K$ number of elements, and maps to the $K$ dimensional **0-1** space, which is also a vector of $K$ elements; in other words, given an input vector $\\mathbf{z} \\in \\mathbb{R}^K$, the soft(arg)max maps it to $\\sigma(\\mathbf{z}) \\in (0,1)^K$.\n",
    "\n",
    "Now we break down what the soft(arg)max function actually does. It applies the standard exponential function to **each** element $z_i$ of the input vector $\\mathbf{z}$ and normalizes these values by dividing by the sum of all these exponentials; this normalization ensures that the sum of the components of the output vector $\\sigma(\\mathbf z)$ is 1.\n",
    "\n",
    "[^softmax]: https://en.wikipedia.org/wiki/Softmax_function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ced969a-c1e8-41f5-8afb-9b3c9675599c",
   "metadata": {},
   "source": [
    "After applying softmax to the logits we have:\n",
    "\n",
    "- **y_prob = z_softargmax:** $$\\yprob = \\textbf{z_softargmax} = \\begin{bmatrix} 0.09 & 0.2447 & 0.6652 \\\\ 0.0159 & 0.1173 & 0.8668\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd356da9-8e9f-4a1a-9041-51cada696935",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_softargmax(z: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Compute the softargmax of a PyTorch tensor.\n",
    "\n",
    "    Args:\n",
    "        z (torch.Tensor): The PyTorch tensor to compute the softargmax of.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The softargmax of the PyTorch tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    # the output matrix should be the same size as the input matrix\n",
    "    z_softargmax = torch.zeros(size=z.size(), dtype=torch.float32)\n",
    "\n",
    "    for row_index, each_row in enumerate(z):\n",
    "        denominator = torch.sum(torch.exp(each_row))\n",
    "        for element_index, each_element in enumerate(each_row):\n",
    "            z_softargmax[row_index, element_index] = (\n",
    "                torch.exp(each_element) / denominator\n",
    "            )\n",
    "\n",
    "    assert compare_closeness_two_tensors(\n",
    "        z_softargmax, torch.nn.Softmax(dim=1)(z), 1e-15\n",
    "    )\n",
    "    return z_softargmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "004dee86-bc5a-491d-aa57-d86c95a69384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0900, 0.2447, 0.6652],\n",
      "        [0.0159, 0.1173, 0.8668]])\n"
     ]
    }
   ],
   "source": [
    "z_softargmax = compute_softargmax(z_logits)\n",
    "print(z_softargmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0de2292-d72d-46c6-b15e-204377436867",
   "metadata": {},
   "source": [
    "### Categorical Cross Entropy Loss\n",
    "\n",
    "We will start with this because the Binary Cross Entropy Loss is merely a special case of this. Finding the full compact formula for this took me a while since most tutorials cover the binary case.\n",
    "\n",
    "Given $N$ samples, and $C$ classes, the **Categorical Cross Entropy Loss** is the average loss across $N$ samples, given by:\n",
    "\n",
    "$$\\textbf{CE}(\\ytrue, \\yprob) = -\\dfrac{1}{N}\\sum_{i=1}^N\\sum_{c=1}^C \\mathbb{1}_{\\y_{i} \\in C_c} \\log\\left(p_{\\textbf{model}}[\\y_i \\in C_c]\\right)$$\n",
    "\n",
    "where\n",
    "\n",
    "- The outer loop $i$ iterates over $N$ observations/samples.\n",
    "- The inner loop $c$ iterates over $C$ classes.\n",
    "- $\\y_i$ represents the true label (in this formula it should be one-hot encoded) of the $i$-th sample.\n",
    "- $\\mathbb{1}_{y_{i} \\in C_c}$ is an indicator function, simply put, for sample $i$, if the true label $\\y_i$ belongs to the $c$-th category, then we assign a $1$, else $0$. We can see it with an example later.\n",
    "- $\\left(p_{\\textbf{model}}[\\y_i \\in C_c]\\right)$ means the probability predicted by the model for the $i$-th observation that belongs to the $c$-th class category. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dd2f31-e1eb-467b-bf0c-b694ce743837",
   "metadata": {},
   "source": [
    "$$\n",
    "\\ytrue = \\begin{bmatrix}  1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\textbf{z_logits} = \\begin{bmatrix} 1 & 2 & 3 \\\\ 2 & 4 & 6 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\yprob = \\textbf{z_softargmax} = \\begin{bmatrix} 0.09 & 0.2447 & 0.6652 \\\\ 0.0159 & 0.1173 & 0.8668\\end{bmatrix}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38132e76-effc-4550-825d-302005154f8a",
   "metadata": {},
   "source": [
    "- We first look at the first sample, index $i = 1$:\n",
    "    - We have the one-hot encoded label for first sample to be $\\y_1 = \\begin{bmatrix} 1 & 0 & 0 \\end{bmatrix}$. This means the label is a cat since the sequence is cat, dog and pig, and thus 1, 0, 0 corresponds to cat 1, dog 0 and pig 0.\n",
    "    - We have the one-hot encoded probability predicted by the model for the first sample to be $\\hat{\\y_1} = \\begin{bmatrix} 0.09 & 0.2447 & 0.6652 \\end{bmatrix}$. This means the probability associated with this sample $1$ is probability of a cat from the model is $9\\%$, a dog $24.47\\%$ and a pig $66.52\\%$.\n",
    "    \n",
    "---\n",
    "\n",
    "- With these information, we go on to the first outer loop's content:\n",
    "    - $\\sum_{c=1}^C \\mathbb{1}_{\\y_{i} \\in C_c} \\log\\left(p_{\\textbf{model}}[\\y_i \\in C_c]\\right)$\n",
    "    - We are looping through the classes, which in this case is loop from $c=1$ to $c=3$ since $C=3$ (3 classes).\n",
    "    - $c = 1$:\n",
    "        - $\\mathbb{1}_{\\y_{i} \\in C_c} = \\mathbb{1}_{\\y_{1} \\in C_1}$: The true label for the first sample is actually the first class, and hence belongs to the $c=1$ category, so our indicator function returns me a $1$. \n",
    "        - $\\log\\left(p_{\\textbf{model}}[\\y_i \\in C_c]\\right) = \\log\\left(p_{\\textbf{model}}[\\y_1 \\in C_1]\\right)$: Applies the log function (natural log here) to the each probability associated with the class. So in this case, since $c=1$, we apply the log function to the first entry $0.09$. We get $\\log(0.09) = -2.4079$\n",
    "    - $c = 2$:\n",
    "        - $\\mathbb{1}_{\\y_{i} \\in C_c} = \\mathbb{1}_{\\y_{1} \\in C_2}$: The true label for the first sample is actually the first class, and hence does not belong to the $c=2$ category, so our indicator function returns me a $0$. \n",
    "        - Regardless, the log of this probability is $\\log(0.2447) = -1.4076$\n",
    "    - $c = 3$:\n",
    "        - $\\mathbb{1}_{\\y_{i} \\in C_c} = \\mathbb{1}_{\\y_{1} \\in C_3}$: The true label for the first sample is actually the first class, and hence does not belong to the $c=3$ category, so our indicator function returns me a $0$. \n",
    "        - Regardless, the log of this probability is $\\log(0.6652) = -0.4076$\n",
    "    - Lastly, we sum them up and get $-2.4076 + 0 + 0 = -2.4076$, note here we only have the first entry! The second and third are $0$.\n",
    "    - In code, this corresponds to the following:\n",
    "        ```python\n",
    "            # loop = 1\n",
    "            current_sample_loss = 0\n",
    "            for each_y_true_element, each_y_prob_element in zip(\n",
    "                each_y_true_one_hot_vector, each_y_prob_one_hot_vector\n",
    "            ):\n",
    "                # Indicator Function\n",
    "                if each_y_true_element == 1:\n",
    "                    current_sample_loss += -1 * torch.log(each_y_prob_element)\n",
    "                else:\n",
    "                    current_sample_loss += 0\n",
    "        ```\n",
    "    - Bonus: If you realize this is just a vector dot product: $\\begin{bmatrix} 1 & 0 & 0 \\end{bmatrix} \\cdot \\log\\left(\\begin{bmatrix} 0.09 \\\\ 0.2447 \\\\ 0.6652 \\end{bmatrix}\\right) = \\begin{bmatrix} 1 & 0 & 0 \\end{bmatrix} \\cdot \\left(\\begin{bmatrix} -2.4076 \\\\ -1.4076 \\\\ -0.4076 \\end{bmatrix}\\right) = -2.4076$\n",
    "    \n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e23adcb-fe2a-487b-81c7-8f966ef0f9ba",
   "metadata": {},
   "source": [
    "- We now look at the second sample, index $i = 2$:\n",
    "    - We have the one-hot encoded label for second sample to be $\\y_2 = \\begin{bmatrix} 0 & 0 & 1 \\end{bmatrix}$. This means the label is a pig since the sequence is cat, dog and pig, and thus 0, 0, 1 corresponds to cat 0, dog 0 and pig 1.\n",
    "    - We have the one-hot encoded probability predicted by the model for the second sample to be $\\hat{\\y_2} = \\begin{bmatrix} 0.0159 & 0.1173 & 0.8868 \\end{bmatrix}$. This means the probability associated with this sample $2$ is probability of a cat from the model is $1.59\\%$, a dog $11.73\\%$ and a pig $88.68\\%$.\n",
    "    \n",
    "---\n",
    "\n",
    "- With these information, we go on to the second outer loop's content:\n",
    "    - $\\sum_{c=2}^C \\mathbb{1}_{\\y_{i} \\in C_c} \\log\\left(p_{\\textbf{model}}[\\y_i \\in C_c]\\right)$\n",
    "    - $c = 2$:\n",
    "        - $\\mathbb{1}_{\\y_{i} \\in C_c} = \\mathbb{1}_{\\y_{2} \\in C_1}$: The true label for the second sample is actually the third class, and hence belongs to the $c=3$ category, so our indicator function returns me a $0$. \n",
    "        - $\\log\\left(p_{\\textbf{model}}[\\y_i \\in C_c]\\right)$: Applies the log function (natural log here) to the each probability associated with the class. So in this case, since $c=1$, we apply the log function to the first entry $0.0159$. We get $\\log(0.0159) = -4.1429$\n",
    "    - $c = 2$:\n",
    "        - $\\mathbb{1}_{\\y_{i} \\in C_c} = \\mathbb{1}_{\\y_{2} \\in C_2}$: The true label for the second sample is actually the third class, and hence does not belong to the $c=2$ category, so our indicator function returns me a $0$. \n",
    "        - Regardless, the log of this probability is $\\log(0.1173) = -2.1429$\n",
    "    - $c = 3$:\n",
    "        - $\\mathbb{1}_{\\y_{i} \\in C_c} = \\mathbb{1}_{\\y_{2} \\in C_3}$: The true label for the second sample is actually the third class, so our indicator function returns me a $1$. \n",
    "        - The log of this probability is $\\log(0.6652) = -0.1429$\n",
    "    - Lastly, we sum them up and get $0 + 0 + (-0.1429) = -0.1429$, note here we only have the third entry! The first and second entries are $0$.\n",
    "    - In code, this corresponds to the following:\n",
    "        ```python\n",
    "            # loop = 2\n",
    "            current_sample_loss = 0\n",
    "            for each_y_true_element, each_y_prob_element in zip(\n",
    "                each_y_true_one_hot_vector, each_y_prob_one_hot_vector\n",
    "            ):\n",
    "                # Indicator Function\n",
    "                if each_y_true_element == 1:\n",
    "                    current_sample_loss += -1 * torch.log(each_y_prob_element)\n",
    "                else:\n",
    "                    current_sample_loss += 0\n",
    "        ```\n",
    "    - Bonus: If you realize this is just a vector dot product: $\\begin{bmatrix} 0 & 0 & 1 \\end{bmatrix} \\cdot \\log\\left(\\begin{bmatrix} 0.0159 \\\\ 0.1173 \\\\ 0.8868\\end{bmatrix}\\right) = \\begin{bmatrix} 0 & 0 & 1 \\end{bmatrix} \\cdot \\left(\\begin{bmatrix} -4.1429 \\\\ -2.1429 \\\\ -0.1429 \\end{bmatrix}\\right) = -0.1429$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a708d7dd-9a0c-4f91-bfc5-0cd9e7d7ab3f",
   "metadata": {},
   "source": [
    "To summarize the whole process:\n",
    "\n",
    "- set `all_samples_loss = 0`\n",
    "- Start Outer Loop:\n",
    "    - loop over first sample `i = 1` (actually index is 0 in python):\n",
    "        - set `current_sample_loss = 0`\n",
    "        - loop over $C=3$ classes:\n",
    "            - when $c = 1$: the loss associated is $-2.4076$. Add this to `current_sample_loss`.\n",
    "            - when $c = 2$: the loss associated is $0$. Add this to `current_sample_loss`.\n",
    "            - when $c = 3$: the loss associated is $0$. Add this to `current_sample_loss`.\n",
    "        - end first loop: update `all_samples_loss` by adding `current_sample_loss` to be `all_samples_loss = -2.4076`.\n",
    "    - loop over second sample `i = 2` (actually index is 1 in python):\n",
    "        - set `current_sample_loss = 0`\n",
    "        - loop over $C=3$ classes:\n",
    "            - when $c = 1$: the loss associated is $0$. Add this to `current_sample_loss`.\n",
    "            - when $c = 2$: the loss associated is $0$. Add this to `current_sample_loss`.\n",
    "            - when $c = 3$: the loss associated is $-0.1429$. Add this to `current_sample_loss`.\n",
    "        - end second loop: update `all_samples_loss` by adding `current_sample_loss` to be `all_samples_loss = -2.4076 + (-0.1429) = -2.5505`. \n",
    "- End all loops: You can multiply by negative $-1$ to make `all_samples_loss` positive and get `all_samples_average_loss = all_samples_loss / num_of_samples = 2.5505 / 2 = 1.2753`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "674d26bd-df5f-445c-82b0-c3ce30c8ef93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_categorical_cross_entropy_loss(\n",
    "    y_true: torch.Tensor, y_prob: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Compute the categorical cross entropy loss between two PyTorch tensors.\n",
    "\n",
    "    Args:\n",
    "        y_true (torch.Tensor): The true labels.\n",
    "        y_prob (torch.Tensor): The predicted labels.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The categorical cross entropy loss.\n",
    "    \"\"\"\n",
    "\n",
    "    all_samples_loss = 0\n",
    "    for each_y_true_one_hot_vector, each_y_prob_one_hot_vector in zip(\n",
    "        y_true, y_prob\n",
    "    ):\n",
    "        current_sample_loss = 0\n",
    "        for each_y_true_element, each_y_prob_element in zip(\n",
    "            each_y_true_one_hot_vector, each_y_prob_one_hot_vector\n",
    "        ):\n",
    "            # in case y_prob has elements that is 0 or very small, then torch.log(0) might go to -inf\n",
    "            each_y_prob_element = torch.clamp(each_y_prob_element, min=1.0e-20, max=1.0-1.0e-20, out=None) \n",
    "            # Indicator Function\n",
    "            if each_y_true_element == 1:\n",
    "                current_sample_loss += -1 * torch.log(each_y_prob_element)\n",
    "            else:\n",
    "                current_sample_loss += 0\n",
    "\n",
    "        all_samples_loss += current_sample_loss\n",
    "\n",
    "    all_samples_average_loss = all_samples_loss / y_true.shape[0]\n",
    "    return all_samples_average_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e48669-aae7-466a-9e7b-18a8b4184864",
   "metadata": {},
   "source": [
    "#### Using Dot Product to Calculate\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\textbf{CE}(\\ytrue, \\yprob) &= -\\dfrac{1}{N}\\sum_{i=1}^N\\sum_{c=1}^C \\mathbb{1}_{\\y_{i} \\in C_c} \\log\\left(p_{\\textbf{model}}[\\y_i \\in C_c]\\right)\\\\\n",
    "                            &= \\textbf{SUM}\\left[\\textbf{diag}\\left(\\ytrue \\cdot -\\log(\\yprob)^\\top\\right)\\right]\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7ccb38-0708-4464-970b-5e403f08d062",
   "metadata": {},
   "source": [
    "We can easily see \n",
    "\n",
    "\n",
    "$$\n",
    "\\ytrue = \\begin{bmatrix}  1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\textbf{z_logits} = \\begin{bmatrix} 1 & 2 & 3 \\\\ 2 & 4 & 6 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\yprob = \\textbf{z_softargmax} = \\begin{bmatrix} 0.09 & 0.2447 & 0.6652 \\\\ 0.0159 & 0.1173 & 0.8668\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\\log(\\yprob) = \\begin{bmatrix} 2.4076 & 1.4076 & 0.4076 \\\\ 4.1429 & 2.1429 & 0.1429 \\end{bmatrix}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a0eb1c-122d-4e2f-8365-dbf6f034993b",
   "metadata": {},
   "source": [
    "$$\n",
    "\\ytrue \\cdot -\\log(\\yprob)^\\top = \\begin{bmatrix}  1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} \\cdot \\begin{bmatrix} 2.4076 & 4.1429 \\\\ 1.4076 & 2.1429 \\\\ 0.4076  & 0.1429 \\end{bmatrix} = \\begin{bmatrix} 2.4076 & 4.1429 \\\\ 0.4076 & 0.1429 \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4faa44-7ded-442b-818b-8067fe941065",
   "metadata": {},
   "source": [
    "The matrix $\\ytrue \\cdot -\\log(\\yprob)^\\top$ diagonals are what we need, where we sum them up and divide by the number of samples. That is $\\frac{2.4076+0.1429}{2} = \\frac{2.5505}{2} = 1.2753$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9b5ca4-c851-496c-b20a-65d25200b9aa",
   "metadata": {},
   "source": [
    "This makes sense because the one hot encoded $\\ytrue$ vector guarantees only the indicator functions 1 gets activated and the rest gets zeroed out. Furthermore, we are only interested in the diagonal of the matrix as we are only interested in the dot product between the $i$-th row and the $i$-th column of $\\ytrue$ and $-\\log(\\yprob)^\\top$ respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "68b564c7-5e7d-4aa0-abb7-d780c1cf1ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_categorical_cross_entropy_loss_dot_product(\n",
    "    y_true: torch.Tensor, y_prob: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Compute the categorical cross entropy loss between two PyTorch tensors using dot product.\n",
    "\n",
    "    Args:\n",
    "        y_true (torch.Tensor): The true labels in one-hot form.\n",
    "        y_prob (torch.Tensor): The predicted labels in one-hot form.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The categorical cross entropy loss.\n",
    "    \"\"\"\n",
    "    m = torch.matmul(y_true.float(), torch.neg(torch.log(y_prob.float()).T))\n",
    "    all_loss_vector = torch.diagonal(m, 0)\n",
    "    all_loss_sum = torch.sum(all_loss_vector, dim=0)\n",
    "    average_loss = all_loss_sum / y_true.shape[0]\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b475f7f4-3844-4741-bb24-b5ef4212056c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2753)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_categorical_cross_entropy_loss(y_true = y_true_ohe, y_prob = compute_softargmax(z_logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "561bcbf4-4f91-43d8-9ace-bc3ca008c1e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2753)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_categorical_cross_entropy_loss_dot_product(y_true = y_true_ohe, y_prob = compute_softargmax(z_logits))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
