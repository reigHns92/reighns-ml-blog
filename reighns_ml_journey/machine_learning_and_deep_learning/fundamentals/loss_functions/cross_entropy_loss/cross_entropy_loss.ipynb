{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e167d03c-63fd-45ad-9df9-d157c5e8e2aa",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "\\newcommand{\\ytrue}{\\mathbf{y_{\\textbf{true}}}}\n",
    "\\newcommand{\\yprob}{\\mathbf{y_{\\textbf{prob}}}}\n",
    "\\newcommand{\\y}{\\mathbf{y}}\n",
    "\\newcommand{\\x}{\\mathbf{x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dd40b6-14c7-493d-aad6-46c395423767",
   "metadata": {},
   "source": [
    "## Cross-Entropy as a Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de647cf-bf8d-4c5c-8d9e-d969dd1ab584",
   "metadata": {},
   "source": [
    "### Intuition\n",
    "\n",
    "We need to make sense of entropy in the form of a loss function, we have to just enhance our thinking a little.\n",
    "\n",
    "We define our target to be a one-hot encoded vector of class 0 and 1.\n",
    "\n",
    "```python\n",
    "target = [0, 1]\n",
    "```\n",
    "\n",
    "Intuitively, take the cat vs dog binary classification again, we made 11 predictions for ONLY ONE query image using different model, and find that after going through many layers, the **softmax** predictions on the logits are as such:\n",
    "\n",
    "```python\n",
    "[\n",
    "    [0.0, 1.0],\n",
    "    [0.1, 0.9],\n",
    "    [0.2, 0.8],\n",
    "    [0.3, 0.7],\n",
    "    [0.4, 0.6],\n",
    "    [0.5, 0.5],\n",
    "    [0.6, 0.4],\n",
    "    [0.7, 0.3],\n",
    "    [0.8, 0.2],\n",
    "    [0.9, 0.1],\n",
    "    [1.0, 0.0],\n",
    "]\n",
    "```\n",
    "\n",
    "where the first index corresponds to the logits of class 0 and second index corresponds to the logits of class 1. \n",
    "\n",
    "\n",
    "For example, `[1, 0]` means the model is 100 percent confident the prediction is a class 0 (cat), and obviously we need to punish the model for spitting nonsense like this.\n",
    "\n",
    "\n",
    "As we can see in the `binary_cross_entropy` function below, we only need to add up two things. And note that we are hinging on class 1 and therefore `y_true[0] * log(y_pred[0]+eps)` goes to 0 as we are just relying on our feedback of probability of class 1.\n",
    "\n",
    "And in our graph, we can see that as predictions gets more wrong, meaning to say, if the query image is a dog, but our predictions is `[1, 0]`, which says it is a cat, our entropy loss will blow up to very high because \n",
    "\n",
    "```python\n",
    "y_true[1] * log(y_pred[1]+eps) -> 1 * log(1+eps) -> almost infinity\n",
    "```\n",
    "\n",
    "Note again we do not calculate for class 0 because \n",
    "\n",
    "1. We one-hot encoded.\n",
    "2. We only look at class 1's probability and that's enough as we can deduce class 0's probability anyways.\n",
    "\n",
    "And conversely, note how the entropy loss goes to 0 if our prediction is say `[0, 1]`. In general, as our probability for the query image gets close to 1, or in agreement with our class, then our entropy loss becomes smaller.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf4699d-729a-4d0e-a6d4-ccd9ab4aed3e",
   "metadata": {},
   "source": [
    "### Cross-Entropy on one Example\n",
    "\n",
    "We first understand the idea and intuition of **Cross-Entropy Loss** on one single example. Consider a dataset of cat (class 0) and dogs (class 1) where after one hot encoding we have class 0 to be $[1, 0]$ and class 1 to be $[0, 1]$.\n",
    "\n",
    "We are given the following:\n",
    "\n",
    "- $\\mathcal{D}$: The dataset.\n",
    "- $\\mathbf{x}_q$: One single query image (i.e one image only). This can be a **random variable**.\n",
    "- $\\mathbf{y}_q$: The corresponding label - dog (class 1) which is $[0, 1]$. \n",
    "- $P$: The probability distribution for the ground truth target - which is $[0, 1]$, one can understand it as the distribution where cat's probability is 0, and dog's probability is 1. \n",
    "- $Q$: The probability distribtion of the estimate on the $\\mathbf{x}_q$, which is say, $[0.1, 0.9]$. \n",
    "\n",
    "Thus, we can compute the cross entropy loss of this single image by:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3263e62-92db-4fa4-921b-7263fabd7067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.10.1+cpu'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import torch.nn.functional as F\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e06e77-bbdb-4960-9315-ad1e29aa21ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_to_np(tensor: torch.Tensor) -> np.ndarray:\n",
    "    \"\"\"Convert a PyTorch tensor to a numpy array.\n",
    "\n",
    "    Args:\n",
    "        tensor (torch.Tensor): The PyTorch tensor to convert.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The converted numpy array.\n",
    "    \"\"\"\n",
    "    return tensor.detach().cpu().numpy()\n",
    "\n",
    "def compare_equality_two_tensors(\n",
    "    tensor1: torch.Tensor, tensor2: torch.Tensor\n",
    ") -> bool:\n",
    "    \"\"\"Compare two PyTorch tensors for equality.\n",
    "\n",
    "    Args:\n",
    "        tensor1 (torch.Tensor): The first PyTorch tensor to compare.\n",
    "        tensor2 (torch.Tensor): The second PyTorch tensor to compare.\n",
    "\n",
    "    Returns:\n",
    "        bool: Whether the two tensors are equal.\n",
    "    \"\"\"\n",
    "    if torch.all(torch.eq(tensor1, tensor2)):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def compare_closeness_two_tensors(\n",
    "    tensor1: torch.Tensor,\n",
    "    tensor2: torch.Tensor,\n",
    "    epsilon: float,\n",
    "    *args,\n",
    "    **kwargs\n",
    ") -> bool:\n",
    "    \"\"\"Compare two PyTorch tensors for closeness.\n",
    "\n",
    "    Args:\n",
    "        tensor1 (torch.Tensor): The first PyTorch tensor to compare.\n",
    "        tensor2 (torch.Tensor): The second PyTorch tensor to compare.\n",
    "        epsilon (float): The epsilon value to use for closeness.\n",
    "\n",
    "    Returns:\n",
    "        bool: Whether the two tensors are close.\n",
    "    \"\"\"\n",
    "    if torch.allclose(tensor1, tensor2, atol=epsilon, *args, **kwargs):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44d56729-3ae3-4347-832e-9ccf27617be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_logits = torch.tensor([[1, 2, 3], [2, 4, 6]], dtype=torch.float32)\n",
    "\n",
    "y_true = torch.tensor([0, 2], dtype=torch.long)\n",
    "\n",
    "y_true_ohe = torch.tensor([[1, 0, 0], [0, 0, 1]], dtype=torch.long)\n",
    "compare_equality_two_tensors(y_true_ohe, torch.nn.functional.one_hot(y_true, num_classes=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f80bd57-1a68-4f3f-86b1-fb9f22cfb65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_softargmax(z: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Compute the softargmax of a PyTorch tensor.\n",
    "\n",
    "    Args:\n",
    "        z (torch.Tensor): The PyTorch tensor to compute the softargmax of.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The softargmax of the PyTorch tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    # the output matrix should be the same size as the input matrix\n",
    "    z_softargmax = torch.zeros(size=z.size(), dtype=torch.float32)\n",
    "\n",
    "    for row_index, each_row in enumerate(z):\n",
    "        denominator = torch.sum(torch.exp(each_row))\n",
    "        for element_index, each_element in enumerate(each_row):\n",
    "            z_softargmax[row_index, element_index] = (\n",
    "                torch.exp(each_element) / denominator\n",
    "            )\n",
    "\n",
    "    assert compare_closeness_two_tensors(\n",
    "        z_softargmax, torch.nn.Softmax(dim=1)(z), 1e-15\n",
    "    )\n",
    "    return z_softargmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f01768ae-a7c0-4eff-903c-cb6982148a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_softargmax = compute_softargmax(z_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9427b9a5-d3ba-4738-9a4e-fe7b58d1f2fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2753)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.CrossEntropyLoss()(z_logits, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "004dee86-bc5a-491d-aa57-d86c95a69384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [2., 4., 6.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fdd531ab-4994-4d0c-8006-2708d8697740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0900, 0.2447, 0.6652],\n",
       "        [0.0159, 0.1173, 0.8668]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_softargmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51a3cf2e-6404-4e6d-b654-c61189ab4ac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0],\n",
       "        [0, 0, 1]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true_ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c85eb40c-5ba8-4d17-83a8-0b053b5fbba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.4076, 4.1429],\n",
       "        [1.4076, 2.1429],\n",
       "        [0.4076, 0.1429]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(-1 * torch.log(z_softargmax)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0fdcb706-17fd-4a48-99c1-9108accb03a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.4076, 4.1429],\n",
       "        [0.4076, 0.1429]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = y_true_ohe.float().matmul((-1 * torch.log(z_softargmax)).T)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "06ab729d-c9ce-4589-99a3-5d9ff01d86ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.5505)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.diagonal(m, 0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0de2292-d72d-46c6-b15e-204377436867",
   "metadata": {},
   "source": [
    "### Categorical Cross Entropy Loss\n",
    "\n",
    "We will start with this because the Binary Cross Entropy Loss is merely a special case of this. Finding the full compact formula for this took me a while since most tutorials cover the binary case.\n",
    "\n",
    "Given $N$ samples, and $C$ classes, the **Categorical Cross Entropy Loss** is the average loss across $N$ samples, given by:\n",
    "\n",
    "$$\\textbf{CE}(\\ytrue, \\yprob) = -\\dfrac{1}{N}\\sum_{i=1}^N\\sum_{c=1}^C \\mathbb{1}_{\\y_{i} \\in C_c} \\log\\left(p_{\\textbf{model}}[\\y_i \\in C_c]\\right)$$\n",
    "\n",
    "where\n",
    "\n",
    "- The outer loop $i$ iterates over $N$ observations/samples.\n",
    "- The inner loop $c$ iterates over $C$ classes.\n",
    "- $\\y_i$ represents the true label (in this formula it should be one-hot encoded) of the $i$-th sample.\n",
    "- $\\mathbb{1}_{y_{i} \\in C_c}$ is an indicator function, simply put, for sample $i$, if the true label $\\y_i$ belongs to the $c$-th category, then we assign a $1$, else $0$. We can see it with an example later.\n",
    "- $\\left(p_{\\textbf{model}}[\\y_i \\in C_c]\\right)$ means the probability predicted by the model for the $i$-th observation that belongs to the $c$-th class category. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dd2f31-e1eb-467b-bf0c-b694ce743837",
   "metadata": {},
   "source": [
    "$$\n",
    "\\ytrue = \\begin{bmatrix}  1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\textbf{z_logits} = \\begin{bmatrix} 1 & 2 & 3 \\\\ 2 & 4 & 6 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\yprob = \\textbf{z_softargmax} = \\begin{bmatrix} 0.09 & 0.2447 & 0.6652 \\\\ 0.0159 & 0.1173 & 0.8668\\end{bmatrix}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38132e76-effc-4550-825d-302005154f8a",
   "metadata": {},
   "source": [
    "- We first look at the first sample, index $i = 1$:\n",
    "    - We have the one-hot encoded label for first sample to be $\\y_1 = \\begin{bmatrix} 1 & 0 & 0 \\end{bmatrix}$. This means the label is a cat since the sequence is cat, dog and pig, and thus 1, 0, 0 corresponds to cat 1, dog 0 and pig 0.\n",
    "    - We have the one-hot encoded probability predicted by the model for the first sample to be $\\hat{\\y_1} = \\begin{bmatrix} 0.09 & 0.2447 & 0.6652 \\end{bmatrix}$. This means the probability associated with this sample $1$ is probability of a cat from the model is $9\\%$, a dog $24.47\\%$ and a pig $66.52\\%$.\n",
    "    \n",
    "---\n",
    "\n",
    "- With these information, we go on to the first outer loop's content:\n",
    "    - $\\sum_{c=1}^C \\mathbb{1}_{\\y_{i} \\in C_c} \\log\\left(p_{\\textbf{model}}[\\y_i \\in C_c]\\right)$\n",
    "    - We are looping through the classes, which in this case is loop from $c=1$ to $c=3$ since $C=3$ (3 classes).\n",
    "    - $c = 1$:\n",
    "        - $\\mathbb{1}_{\\y_{i} \\in C_c} = \\mathbb{1}_{\\y_{1} \\in C_1}$: The true label for the first sample is actually the first class, and hence belongs to the $c=1$ category, so our indicator function returns me a $1$. \n",
    "        - $\\log\\left(p_{\\textbf{model}}[\\y_i \\in C_c]\\right) = \\log\\left(p_{\\textbf{model}}[\\y_1 \\in C_1]\\right)$: Applies the log function (natural log here) to the each probability associated with the class. So in this case, since $c=1$, we apply the log function to the first entry $0.09$. We get $\\log(0.09) = -2.4079$\n",
    "    - $c = 2$:\n",
    "        - $\\mathbb{1}_{\\y_{i} \\in C_c} = \\mathbb{1}_{\\y_{1} \\in C_2}$: The true label for the first sample is actually the first class, and hence does not belong to the $c=2$ category, so our indicator function returns me a $0$. \n",
    "        - Regardless, the log of this probability is $\\log(0.2447) = -1.4076$\n",
    "    - $c = 3$:\n",
    "        - $\\mathbb{1}_{\\y_{i} \\in C_c} = \\mathbb{1}_{\\y_{1} \\in C_3}$: The true label for the first sample is actually the first class, and hence does not belong to the $c=3$ category, so our indicator function returns me a $0$. \n",
    "        - Regardless, the log of this probability is $\\log(0.6652) = -0.4076$\n",
    "    - Lastly, we sum them up and get $-2.4076 + 0 + 0 = -2.4076$, note here we only have the first entry! The second and third are $0$.\n",
    "    - In code, this corresponds to the following:\n",
    "        ```python\n",
    "            # loop = 1\n",
    "            current_sample_loss = 0\n",
    "            for each_y_true_element, each_y_prob_element in zip(\n",
    "                each_y_true_one_hot_vector, each_y_prob_one_hot_vector\n",
    "            ):\n",
    "                # Indicator Function\n",
    "                if each_y_true_element == 1:\n",
    "                    current_sample_loss += -1 * torch.log(each_y_prob_element)\n",
    "                else:\n",
    "                    current_sample_loss += 0\n",
    "        ```\n",
    "    - Bonus: If you realize this is just a vector dot product: $\\begin{bmatrix} 1 & 0 & 0 \\end{bmatrix} \\cdot \\log\\left(\\begin{bmatrix} 0.09 \\\\ 0.2447 \\\\ 0.6652 \\end{bmatrix}\\right) = \\begin{bmatrix} 1 & 0 & 0 \\end{bmatrix} \\cdot \\left(\\begin{bmatrix} -2.4076 \\\\ -1.4076 \\\\ -0.4076 \\end{bmatrix}\\right) = -2.4076$\n",
    "    \n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e23adcb-fe2a-487b-81c7-8f966ef0f9ba",
   "metadata": {},
   "source": [
    "- We now look at the second sample, index $i = 2$:\n",
    "    - We have the one-hot encoded label for second sample to be $\\y_2 = \\begin{bmatrix} 0 & 0 & 1 \\end{bmatrix}$. This means the label is a pig since the sequence is cat, dog and pig, and thus 0, 0, 1 corresponds to cat 0, dog 0 and pig 1.\n",
    "    - We have the one-hot encoded probability predicted by the model for the second sample to be $\\hat{\\y_2} = \\begin{bmatrix} 0.0159 & 0.1173 & 0.8868 \\end{bmatrix}$. This means the probability associated with this sample $2$ is probability of a cat from the model is $1.59\\%$, a dog $11.73\\%$ and a pig $88.68\\%$.\n",
    "    \n",
    "---\n",
    "\n",
    "- With these information, we go on to the second outer loop's content:\n",
    "    - $\\sum_{c=2}^C \\mathbb{1}_{\\y_{i} \\in C_c} \\log\\left(p_{\\textbf{model}}[\\y_i \\in C_c]\\right)$\n",
    "    - $c = 2$:\n",
    "        - $\\mathbb{1}_{\\y_{i} \\in C_c} = \\mathbb{1}_{\\y_{2} \\in C_1}$: The true label for the second sample is actually the third class, and hence belongs to the $c=3$ category, so our indicator function returns me a $0$. \n",
    "        - $\\log\\left(p_{\\textbf{model}}[\\y_i \\in C_c]\\right)$: Applies the log function (natural log here) to the each probability associated with the class. So in this case, since $c=1$, we apply the log function to the first entry $0.0159$. We get $\\log(0.0159) = -4.1429$\n",
    "    - $c = 2$:\n",
    "        - $\\mathbb{1}_{\\y_{i} \\in C_c} = \\mathbb{1}_{\\y_{2} \\in C_2}$: The true label for the second sample is actually the third class, and hence does not belong to the $c=2$ category, so our indicator function returns me a $0$. \n",
    "        - Regardless, the log of this probability is $\\log(0.1173) = -2.1429$\n",
    "    - $c = 3$:\n",
    "        - $\\mathbb{1}_{\\y_{i} \\in C_c} = \\mathbb{1}_{\\y_{2} \\in C_3}$: The true label for the second sample is actually the third class, so our indicator function returns me a $1$. \n",
    "        - The log of this probability is $\\log(0.6652) = -0.1429$\n",
    "    - Lastly, we sum them up and get $0 + 0 + (-0.1429) = -0.1429$, note here we only have the third entry! The first and second entries are $0$.\n",
    "    - In code, this corresponds to the following:\n",
    "        ```python\n",
    "            # loop = 2\n",
    "            current_sample_loss = 0\n",
    "            for each_y_true_element, each_y_prob_element in zip(\n",
    "                each_y_true_one_hot_vector, each_y_prob_one_hot_vector\n",
    "            ):\n",
    "                # Indicator Function\n",
    "                if each_y_true_element == 1:\n",
    "                    current_sample_loss += -1 * torch.log(each_y_prob_element)\n",
    "                else:\n",
    "                    current_sample_loss += 0\n",
    "        ```\n",
    "    - Bonus: If you realize this is just a vector dot product: $\\begin{bmatrix} 0 & 0 & 1 \\end{bmatrix} \\cdot \\log\\left(\\begin{bmatrix} 0.0159 \\\\ 0.1173 \\\\ 0.8868\\end{bmatrix}\\right) = \\begin{bmatrix} 0 & 0 & 1 \\end{bmatrix} \\cdot \\left(\\begin{bmatrix} -4.1429 \\\\ -2.1429 \\\\ -0.1429 \\end{bmatrix}\\right) = -0.1429$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a708d7dd-9a0c-4f91-bfc5-0cd9e7d7ab3f",
   "metadata": {},
   "source": [
    "To summarize the whole process:\n",
    "\n",
    "- set `all_samples_loss = 0`\n",
    "- Start Outer Loop:\n",
    "    - loop over first sample `i = 1` (actually index is 0 in python):\n",
    "        - set `current_sample_loss = 0`\n",
    "        - loop over $C=3$ classes:\n",
    "            - when $c = 1$: the loss associated is $-2.4076$. Add this to `current_sample_loss`.\n",
    "            - when $c = 2$: the loss associated is $0$. Add this to `current_sample_loss`.\n",
    "            - when $c = 3$: the loss associated is $0$. Add this to `current_sample_loss`.\n",
    "        - end first loop: update `all_samples_loss` by adding `current_sample_loss` to be `all_samples_loss = -2.4076`.\n",
    "    - loop over second sample `i = 2` (actually index is 1 in python):\n",
    "        - set `current_sample_loss = 0`\n",
    "        - loop over $C=3$ classes:\n",
    "            - when $c = 1$: the loss associated is $0$. Add this to `current_sample_loss`.\n",
    "            - when $c = 2$: the loss associated is $0$. Add this to `current_sample_loss`.\n",
    "            - when $c = 3$: the loss associated is $-0.1429$. Add this to `current_sample_loss`.\n",
    "        - end second loop: update `all_samples_loss` by adding `current_sample_loss` to be `all_samples_loss = -2.4076 + (-0.1429) = -2.5505`. \n",
    "- End all loops: You can multiply by negative $-1$ to make `all_samples_loss` positive and get `all_samples_average_loss = all_samples_loss / num_of_samples = 2.5505 / 2 = 1.2753`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "674d26bd-df5f-445c-82b0-c3ce30c8ef93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_categorical_cross_entropy_loss(\n",
    "    y_true: torch.Tensor, y_prob: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Compute the categorical cross entropy loss between two PyTorch tensors.\n",
    "\n",
    "    Args:\n",
    "        y_true (torch.Tensor): The true labels.\n",
    "        y_prob (torch.Tensor): The predicted labels.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The categorical cross entropy loss.\n",
    "    \"\"\"\n",
    "\n",
    "    all_samples_loss = 0\n",
    "    for each_y_true_one_hot_vector, each_y_prob_one_hot_vector in zip(\n",
    "        y_true, y_prob\n",
    "    ):\n",
    "        current_sample_loss = 0\n",
    "        for each_y_true_element, each_y_prob_element in zip(\n",
    "            each_y_true_one_hot_vector, each_y_prob_one_hot_vector\n",
    "        ):\n",
    "            # Indicator Function\n",
    "            if each_y_true_element == 1:\n",
    "                current_sample_loss += -1 * torch.log(each_y_prob_element)\n",
    "            else:\n",
    "                current_sample_loss += 0\n",
    "\n",
    "        all_samples_loss += current_sample_loss\n",
    "\n",
    "    all_samples_average_loss = all_samples_loss / y_true.shape[0]\n",
    "    return all_samples_average_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e48669-aae7-466a-9e7b-18a8b4184864",
   "metadata": {},
   "source": [
    "#### Using Dot Product to Calculate\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\textbf{CE}(\\ytrue, \\yprob) &= -\\dfrac{1}{N}\\sum_{i=1}^N\\sum_{c=1}^C \\mathbb{1}_{\\y_{i} \\in C_c} \\log\\left(p_{\\textbf{model}}[\\y_i \\in C_c]\\right)\\\\\n",
    "                            &= \\textbf{SUM}\\left[\\textbf{diag}\\left(\\ytrue \\cdot -\\log(\\yprob)^\\top\\right)\\right]\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7ccb38-0708-4464-970b-5e403f08d062",
   "metadata": {},
   "source": [
    "We can easily see \n",
    "\n",
    "\n",
    "$$\n",
    "\\ytrue = \\begin{bmatrix}  1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\textbf{z_logits} = \\begin{bmatrix} 1 & 2 & 3 \\\\ 2 & 4 & 6 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\yprob = \\textbf{z_softargmax} = \\begin{bmatrix} 0.09 & 0.2447 & 0.6652 \\\\ 0.0159 & 0.1173 & 0.8668\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\\log(\\yprob) = \\begin{bmatrix} 2.4076 & 1.4076 & 0.4076 \\\\ 4.1429 & 2.1429 & 0.1429 \\end{bmatrix}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a0eb1c-122d-4e2f-8365-dbf6f034993b",
   "metadata": {},
   "source": [
    "$$\n",
    "\\ytrue \\cdot -\\log(\\yprob)^\\top = \\begin{bmatrix}  1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} \\cdot \\begin{bmatrix} 2.4076 & 4.1429 \\\\ 1.4076 & 2.1429 \\\\ 0.4076  & 0.1429 \\end{bmatrix} = \\begin{bmatrix} 2.4076 & 4.1429 \\\\ 0.4076 & 0.1429 \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4faa44-7ded-442b-818b-8067fe941065",
   "metadata": {},
   "source": [
    "The matrix $\\ytrue \\cdot -\\log(\\yprob)^\\top$ diagonals are what we need, where we sum them up and divide by the number of samples. That is $\\frac{2.4076+0.1429}{2} = \\frac{2.5505}{2} = 1.2753$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9b5ca4-c851-496c-b20a-65d25200b9aa",
   "metadata": {},
   "source": [
    "This makes sense because the one hot encoded $\\ytrue$ vector guarantees only the indicator functions 1 gets activated and the rest gets zeroed out. Furthermore, we are only interested in the diagonal of the matrix as we are only interested in the dot product between the $i$-th row and the $i$-th column of $\\ytrue$ and $-\\log(\\yprob)^\\top$ respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "68b564c7-5e7d-4aa0-abb7-d780c1cf1ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_categorical_cross_entropy_loss_dot_product(\n",
    "    y_true: torch.Tensor, y_prob: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Compute the categorical cross entropy loss between two PyTorch tensors using dot product.\n",
    "\n",
    "    Args:\n",
    "        y_true (torch.Tensor): The true labels in one-hot form.\n",
    "        y_prob (torch.Tensor): The predicted labels in one-hot form.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The categorical cross entropy loss.\n",
    "    \"\"\"\n",
    "    m = torch.matmul(y_true.float(), torch.neg(torch.log(y_prob.float()).T))\n",
    "    all_loss_vector = torch.diagonal(m, 0)\n",
    "    all_loss_sum = torch.sum(all_loss_vector, dim=0)\n",
    "    average_loss = all_loss_sum / y_true.shape[0]\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b475f7f4-3844-4741-bb24-b5ef4212056c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2753)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_categorical_cross_entropy_loss(y_true = y_true_ohe, y_prob = compute_softargmax(z_logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "561bcbf4-4f91-43d8-9ace-bc3ca008c1e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2753)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_categorical_cross_entropy_loss_dot_product(y_true = y_true_ohe, y_prob = compute_softargmax(z_logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2cdf9871-f8e9-48be-9f1f-b5a54b3d00a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0900, 0.2447, 0.6652],\n",
       "        [0.0159, 0.1173, 0.8668]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_softargmax(z_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8fe23aff-6460-459b-b272-7e75e6d47d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y_true: np.ndarray, y_pred: np.ndarray, epsilon: float = 1e-12):\n",
    "    \"\"\"\n",
    "        https://stackoverflow.com/questions/47377222/what-is-the-problem-with-my-implementation-of-the-cross-entropy-function\n",
    "        Computes cross entropy between targets (encoded as one-hot vectors)\n",
    "        and y_pred.\n",
    "        Input: y_pred (N, k) ndarray\n",
    "               y_true (N, k) ndarray\n",
    "        Returns: scalar\n",
    "        predictions = np.array([[0.25,0.25,0.25,0.25],\n",
    "                            [0.01,0.01,0.01,0.96]])\n",
    "    targets = np.array([[0,0,0,1],\n",
    "                       [0,0,0,1]])\n",
    "                       ans = 0.71355817782  #Correct answer\n",
    "    x = cross_entropy(predictions, targets)\n",
    "    print(np.isclose(x,ans))\n",
    "    \"\"\"\n",
    "    y_pred = np.clip(y_pred, epsilon, 1.0 - epsilon)\n",
    "    # take note that y_pred is of shape 1 x n_samples as stated in our framework\n",
    "    n_samples = y_pred.shape[1]\n",
    "\n",
    "    # cross entropy function\n",
    "    cross_entropy_function = y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)\n",
    "    # cross entropy function here is same shape as y_true and y_pred since we are\n",
    "    # just performing element wise operations on both of them.\n",
    "    assert cross_entropy_function.shape == (1, n_samples)\n",
    "\n",
    "    # we sum up all the loss for each individual sample\n",
    "    total_cross_entropy_loss = -np.sum(cross_entropy_function, axis=1)\n",
    "    assert total_cross_entropy_loss.shape == (1,)\n",
    "\n",
    "    # we then average out the total loss across m samples, but we squeeze it to\n",
    "    # make it a scalar; squeeze along axis = None since there is no column axix\n",
    "    average_cross_entropy_loss = np.squeeze(total_cross_entropy_loss / n_samples, axis=None)\n",
    "\n",
    "    # cross_entropy_loss = -np.sum(y_true * np.log(y_pred)) / n_samples\n",
    "    # print(np.isclose(average_cross_entropy_loss, cross_entropy_loss))\n",
    "    return average_cross_entropy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f999630f-db1e-41a3-918f-cc0425a7587e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.01005034)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy(y_true=np.asarray([[0, 1]]), y_pred=np.asarray([[0.01, 0.99]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67aa9de6-36bd-4b9d-bb92-cc6bd3391ca0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4924ee2-7ac5-4c32-8e91-549bad965275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from math import log\n",
    "\n",
    "# calculate cross-entropy\n",
    "def cross_entropy(p, q, ets=1e-15):\n",
    "    return -sum([p[i]*log(q[i]+ets) for i in range(len(p))])\n",
    "\n",
    "def binary_cross_entropy(y_true, y_pred, eps=1e-15):\n",
    "    return -(y_true[0] * log(y_pred[0]+eps) + y_true[1] * log(y_pred[1]+eps))\n",
    "\n",
    "# define the target distribution for two events\n",
    "target = [0.0, 1.0]\n",
    "# define probabilities for the first event\n",
    "probs = [\n",
    "    [0.0, 1.0], # cat is 0% and dog is 100% confidence\n",
    "    [0.1, 0.9],\n",
    "    [0.2, 0.8],\n",
    "    [0.3, 0.7],\n",
    "    [0.4, 0.6],\n",
    "    [0.5, 0.5],\n",
    "    [0.6, 0.4],\n",
    "    [0.7, 0.3],\n",
    "    [0.8, 0.2],\n",
    "    [0.9, 0.1],\n",
    "    [1.0, 0.0],\n",
    "]\n",
    "\n",
    "# create probability distributions for the two events\n",
    "# dists = [[1.0 - p, p] for p in probs]\n",
    "# calculate cross-entropy for each distribution\n",
    "ents = [binary_cross_entropy(y_true=target, y_pred=d) for d in probs]\n",
    "# plot probability distribution vs cross-entropy\n",
    "pyplot.plot([p[1] for p in probs], ents, marker='.')\n",
    "pyplot.title('Probability Distribution vs Cross-Entropy')\n",
    "#pyplot.xticks([1-p for p in probs], ['[%.1f,%.1f]'%(d[0],d[1]) for d in dists], rotation=70)\n",
    "pyplot.subplots_adjust(bottom=0.2)\n",
    "pyplot.xlabel('Probability Distribution for Query Image when ground truth is 1')\n",
    "pyplot.ylabel('Cross-Entropy (nats)')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2249b099-9db6-4ba3-815f-43fa50925436",
   "metadata": {},
   "source": [
    "- [analytics-vidhya-entropy-loss](https://medium.com/analytics-vidhya/understanding-entropy-the-golden-measurement-of-machine-learning-4ea97c663dc3#:~:text=By%20using%20entropy%20in%20machine,be%20desired%20in%20model%2Dbuilding.)\n",
    "- [cross-entropy-loss-machine-learning-mastery](https://machinelearningmastery.com/cross-entropy-for-machine-learning/)\n",
    "- [entropy-how-decision-trees-make-decisions](https://towardsdatascience.com/entropy-how-decision-trees-make-decisions-2946b9c18c8)\n",
    "- https://ramsane.github.io/articles/cross-entropy-explained-with-entropy-and-kl-divergence\n",
    "- https://neptune.ai/blog/cross-entropy-loss-and-its-applications-in-deep-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e5e382-da32-4276-a550-b5a19d115dbd",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/41990250/what-is-cross-entropy/41990932\n",
    "https://towardsdatascience.com/cross-entropy-loss-function-f38c4ec8643e\n",
    "https://neptune.ai/blog/cross-entropy-loss-and-its-applications-in-deep-learning\n",
    "https://machinelearningmastery.com/cross-entropy-for-machine-learning/\n",
    "https://d2l.ai/chapter_linear-networks/softmax-regression.html\n",
    "https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/geometry-linear-algebraic-ops.html#invertibility\n",
    "https://leimao.github.io/blog/Cross-Entropy-KL-Divergence-MLE/\n",
    "https://towardsdatascience.com/cross-entropy-for-classification-d98e7f974451\n",
    "https://gist.github.com/yang-zhang/217dcc6ae9171d7a46ce42e215c1fee0\n",
    "https://datascience.stackexchange.com/questions/20296/cross-entropy-loss-explanation\n",
    "https://ramsane.github.io/articles/cross-entropy-explained-with-entropy-and-kl-divergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c673291d-b531-44af-8764-e9a7a6780eb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
